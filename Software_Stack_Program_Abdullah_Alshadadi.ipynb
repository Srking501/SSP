{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG75AmT5Zu5a"
      },
      "source": [
        "# **ABDULLAH ALSHADADI - 190582184**\n",
        "\n",
        "Computer Science (Software engineering) Bsc Student\n",
        "\n",
        "## Dissertation Title:\n",
        "----------------------\n",
        "## Machine Learning (ML) Model for The Centre for Search Research (TCSR) to Find Missing Persons Using Drones Imaging Data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI2gYtMLGvu9"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade mlxtend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXgBLyQrHTiS"
      },
      "outputs": [],
      "source": [
        "import mlxtend\n",
        "print(mlxtend.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4TjHGu7GBf4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from numpy import ndarray\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import datetime\n",
        "import shutil\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "%load_ext tensorboard\n",
        "%matplotlib inline\n",
        "\n",
        "# Demonstration Purposes Only\n",
        "# from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOx8XhKMIAEe"
      },
      "outputs": [],
      "source": [
        "class SubdivisionIsBiggerThanBatch(Exception):\n",
        "    def __init__(self, message, errors=None):\n",
        "        self.message = message\n",
        "        self.errors = errors\n",
        "        super(SubdivisionIsBiggerThanBatch, self).__init__(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkgJSMuOdfor"
      },
      "source": [
        "Enable Google Drive for Uploading and Executing the Program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0bhTDvhdaC9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "DRIVE_PATH = \"/content/drive/\"\n",
        "drive.mount(DRIVE_PATH)\n",
        "DRIVE_PATH = DRIVE_PATH + \"MyDrive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbM217HgXpPH"
      },
      "source": [
        "## **Select Model to Run By Typing True**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBEewDkOXnmK"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "# MODELS TO RUN #\n",
        "#################\n",
        "YOLO_MODEL = True\n",
        "RES_NET_MODEL = True\n",
        "MOBILENET_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHvZ5Y3uGj-f"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "# SET-UP #\n",
        "##########\n",
        "SKIP_DATA_PROCESSING = False\n",
        "CLASSES = [\"Person\", \"No Person\"]\n",
        "PATH_DATA = DRIVE_PATH + \"quick_preview/\"\n",
        "TEST_DATA = PATH_DATA + \"test/\"\n",
        "FINAL_MODELS_PATH = PATH_DATA + \"models/\"\n",
        "BACKUP_TENSOR = PATH_DATA + \"backup/tensorflow/\"\n",
        "NUMPY_SAVE = PATH_DATA + \"numpy/\"\n",
        "NUM_BUF = 30\n",
        "MIN_CONTOUR = 100\n",
        "MAX_CONTOUR = 10000\n",
        "# Use website to visualise how the HSV range colour selected looks  like:\n",
        "# https://wamingo.net/rgbbgr/ (WARNING: it uses (360Degree, 100%, 100%) data)\n",
        "# OpenCV uses (0-179, 0-255, 0-255)\n",
        "#\n",
        "# Trick to convert it:\n",
        "# (half the degree, 255 x 1.0, 255 x 1.0)\n",
        "# (Trick got from https://stackoverflow.com/a/10951189)\n",
        "COLOUR_HSV_RANGE = [  # [lower bound, upper bound]\n",
        "    [np.array([156, 148, 150]), np.array([179, 255, 255])],  # Red Range\n",
        "    [np.array([110, 125, 125]), np.array([150, 255, 255])]  # Blue Range\n",
        "]\n",
        "\n",
        "if os.path.isdir(PATH_DATA) and os.path.exists(PATH_DATA):\n",
        "  pass\n",
        "else:\n",
        "  raise FileNotFoundError(\"Path to Directory \" + PATH_DATA + \" Not Found\")\n",
        "\n",
        "SETUP_DATA_DIRS = [TEST_DATA, FINAL_MODELS_PATH, BACKUP_TENSOR, NUMPY_SAVE]\n",
        "for setup_data_dir in SETUP_DATA_DIRS:\n",
        "  if os.path.isdir(setup_data_dir) and os.path.exists(setup_data_dir):\n",
        "    print(setup_data_dir)\n",
        "    pass\n",
        "  else:\n",
        "    pathlib.Path(setup_data_dir).mkdir(parents=True, exist_ok=True)\n",
        "    print(setup_data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqMNRgPRhkaT"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# Hyper-parameters #\n",
        "####################\n",
        "IMG_SIZE = 416\n",
        "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "EPOCHS = 10\n",
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJE47vlBGpNg"
      },
      "outputs": [],
      "source": [
        "###############\n",
        "# YOLO SET-UP #\n",
        "###############\n",
        "# Directories\n",
        "YOLO_BACKUP = PATH_DATA + \"backup/yolo/\"\n",
        "YOLO_FRAMES = PATH_DATA + \"frames/\"\n",
        "YOLO_METADATA = PATH_DATA + \"metadata/yolo/\"\n",
        "\n",
        "# Files\n",
        "YOLO_MAIN_DATA = YOLO_METADATA + \"main.txt\"\n",
        "YOLO_VALID_DATA = YOLO_METADATA + \"valid.txt\"\n",
        "YOLO_TEST_DATA = YOLO_METADATA + \"test.txt\"\n",
        "YOLO_DATA_FILE = YOLO_METADATA + \"obj.data\"\n",
        "YOLO_NAMES_FILE = YOLO_METADATA + \"obj.names\"\n",
        "\n",
        "# Make the directories\n",
        "YOLO_PATHS = [YOLO_BACKUP, YOLO_FRAMES, YOLO_METADATA]\n",
        "for yolo_path in YOLO_PATHS:\n",
        "  if os.path.isdir(yolo_path) and os.path.exists(yolo_path):\n",
        "    print(yolo_path)\n",
        "    pass\n",
        "  else:\n",
        "    pathlib.Path(yolo_path).mkdir(parents=True, exist_ok=True)\n",
        "    print(yolo_path)\n",
        "\n",
        "# *.data file\n",
        "file_data = open(YOLO_DATA_FILE, \"w\")\n",
        "file_data.write(\"classes={0}\\n\"\n",
        "                \"train={1}\\n\"\n",
        "                \"valid={2}\\n\"\n",
        "                \"names={3}\\n\"\n",
        "                \"backup={4}\\n\"\n",
        "                .format(len(CLASSES),\n",
        "                        YOLO_MAIN_DATA,\n",
        "                        YOLO_VALID_DATA,\n",
        "                        YOLO_NAMES_FILE,\n",
        "                        YOLO_BACKUP))\n",
        "file_data.close()\n",
        "# *.names file\n",
        "file_names = open(YOLO_NAMES_FILE, \"w\")\n",
        "for name in CLASSES:\n",
        "    file_names.write(\"{0}\\n\".format(name))\n",
        "file_names.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hqm2BEKeG0_i"
      },
      "outputs": [],
      "source": [
        "#########################\n",
        "# YOLO Hyper-parameters #\n",
        "#########################\n",
        "YOLO_BATCH = 64\n",
        "YOLO_SUBDIVISION = 32\n",
        "YOLO_MAX_BATCHES = 4000\n",
        "YOLO_LOWER_STEPS = 400\n",
        "YOLO_UPPER_STEPS = 450\n",
        "if YOLO_SUBDIVISION > YOLO_BATCH:\n",
        "    raise SubdivisionIsBiggerThanBatch(str(YOLO_SUBDIVISION) + \" SUBDIVISION is bigger than \" + str(YOLO_BATCH) + \" BATCH\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_sS3B1_O0HB"
      },
      "source": [
        "# DATA PROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQKVKgs4PJG4"
      },
      "source": [
        "Progress bar to see the processing progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYv3c326PHk9"
      },
      "outputs": [],
      "source": [
        "# Code extracted from:\n",
        "#\n",
        "# https://gist.github.com/greenstick/b23e475d2bfdc3a82e34eaa1f6781ee4\n",
        "def print_progress_bar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='â–ˆ', autosize=False):\n",
        "    \"\"\"\n",
        "    Call in a loop to create terminal progress bar\n",
        "    @params:\n",
        "        iteration   - Required  : current iteration (Int)\n",
        "        total       - Required  : total iterations (Int)\n",
        "        prefix      - Optional  : prefix string (Str)\n",
        "        suffix      - Optional  : suffix string (Str)\n",
        "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
        "        length      - Optional  : character length of bar (Int)\n",
        "        fill        - Optional  : bar fill character (Str)\n",
        "        autosize    - Optional  : automatically resize the length of the progress bar to the terminal window (Bool)\n",
        "    \"\"\"\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    styling = '%s |%s| %s%% %s' % (prefix, fill, percent, suffix)\n",
        "    if autosize:\n",
        "        cols, _ = shutil.get_terminal_size(fallback=(length, 1))\n",
        "        length = cols - len(styling)\n",
        "    filled_length = int(length * iteration // total)\n",
        "    bar = fill * filled_length + '-' * (length - filled_length)\n",
        "    print('\\r%s' % styling.replace(fill, bar), end='\\r')\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total:\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr0umwMNOvYb"
      },
      "source": [
        "## **Functions to Run Data processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW-N8K6aY0E1"
      },
      "source": [
        "Masking function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ9dHpJIYnsC"
      },
      "outputs": [],
      "source": [
        "def mask_colour_range(frame: ndarray) -> ndarray:\n",
        "    \"\"\"\n",
        "    Using the HSV colour range to create the masking value for a frame to be masked;\n",
        "    checkout COLOUR_HSV_RANGE above in the setup section\n",
        "    @params:\n",
        "        frame - Required : the current frame that going to be masked (ndarray)\n",
        "    @returns:\n",
        "        The mask frame value (ndarray)\n",
        "    \"\"\"\n",
        "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "    mask = 0\n",
        "    for bounding_range in COLOUR_HSV_RANGE:\n",
        "        #                              lower bound      , upper bound\n",
        "        mask = mask + cv2.inRange(hsv, bounding_range[0], bounding_range[1])\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ylIHyYSY2VE"
      },
      "source": [
        "The heart of the data processing: the loading of the data and data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmVpSWXrO2d8"
      },
      "outputs": [],
      "source": [
        "def load_video(path: str, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    \"\"\"\n",
        "    Loads the video and store into numpy array\n",
        "    @params:\n",
        "        path       - Required                     : location of the video file to load (Str)\n",
        "        max_frames - Default 0                    : used to break if frames is not present (Int)\n",
        "        resize     - Default (IMG_SIZE, IMG_SIZE) : uses the hyperparameter to resize the frame (Tuple(Int, Int))\n",
        "    @returns:\n",
        "        Tuple of 2 lists, one for training data and the other is training labels (Tuple(List, List))\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    frames = []\n",
        "\n",
        "    frames_with_missing_person = []\n",
        "    labels_for_missing_person = []\n",
        "\n",
        "    frames_without_missing_person = []\n",
        "    labels_for_without_missing_person = []\n",
        "\n",
        "    progress = 0\n",
        "    try:\n",
        "        # while len(frames) <= 6000:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            # Clean copy of frame\n",
        "            temp_frame = frame.copy()\n",
        "            frames.append(frame)\n",
        "\n",
        "            #############################\n",
        "            # Labels the missing person #\n",
        "            #############################\n",
        "            mask = mask_colour_range(frame)\n",
        "\n",
        "            kernel = np.ones((30, 15), np.float32) / 225\n",
        "            mask = cv2.filter2D(mask, -1, kernel)\n",
        "\n",
        "            result = cv2.bitwise_and(frame, frame, mask=mask)\n",
        "\n",
        "            if len(frames) % NUM_BUF == 0 or total_frames - len(frame) == total_frames % NUM_BUF:\n",
        "                contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                if len(contours) != 0:\n",
        "                    frames_with_missing_person.append(temp_frame)\n",
        "                    for label in CLASSES:\n",
        "                        label_name = \"Person\"\n",
        "                        if label == label_name:\n",
        "                            labels_for_missing_person.append(CLASSES.index(label_name))\n",
        "                            print(\"Found:\", label_name, \", Label Num:\", CLASSES.index(label_name))\n",
        "                    proces_images(path, contours, frame, temp_frame, progress)\n",
        "                else:\n",
        "                    frames_without_missing_person.append(temp_frame)\n",
        "                    for label in CLASSES:\n",
        "                        label_name = \"No Person\"\n",
        "                        if label == label_name:\n",
        "                            labels_for_without_missing_person.append(CLASSES.index(label_name))\n",
        "                            print(\"Found:\", label_name, \", Label Num:\", CLASSES.index(label_name))\n",
        "                # Demonstration Purposes Only\n",
        "                # cv2.imshow(\"image\", np.hstack([frame, result]))\n",
        "                # cv2_imshow(np.hstack([frame, result]))\n",
        "            #############################\n",
        "\n",
        "            # Demonstration Purposes Only\n",
        "            # cv2.imshow(\"image\", np.hstack([frame, result]))\n",
        "            # cv2_imshow(np.hstack([frame, result]))\n",
        "\n",
        "            if cv2.waitKey(1) == ord('q'):\n",
        "                break\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "\n",
        "            # Progress bar to see the progress of labelling the missing person\n",
        "            progress += 1\n",
        "            if path != 0:\n",
        "                print_progress_bar(progress, total_frames,\n",
        "                                   prefix='Progress:', suffix='Extracting Images',\n",
        "                                   autosize=True)\n",
        "    finally:\n",
        "        cap.release()\n",
        "    training_data = frames_with_missing_person + frames_without_missing_person\n",
        "    training_labels = labels_for_missing_person + labels_for_without_missing_person\n",
        "    return training_data, training_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei89XDLQZBQx"
      },
      "outputs": [],
      "source": [
        "def proces_images(path: str, contours: list, current_frame: ndarray, clean_frame: ndarray, progress: int):\n",
        "    \"\"\"\n",
        "    Adds contours to the masked areas of a frame and if YOLO_MODEL = True, it saves the frames\n",
        "    as *.jpg files with the coordinates of the contours' rectangle area saved as a *.txt file\n",
        "    @params\n",
        "        path          - Required : path to the video (Str)\n",
        "        contours      - Required : list of contours to be used to find the counterArea() and boundingRect(contours) (List)\n",
        "        current_frame - Required : the frame that is going to be processed with contours (ndarray)\n",
        "        clean_frame   - Required : clean version of the current_frame that has not been processed for saving (ndarray)\n",
        "        progress      - Required : the progress count of the frames loaded in to differentiate file names when saving (Int)\n",
        "    \"\"\"\n",
        "    for contours in contours:\n",
        "        if MIN_CONTOUR < cv2.contourArea(contours) < MAX_CONTOUR:\n",
        "            x, y, w, h = cv2.boundingRect(contours)\n",
        "\n",
        "            if YOLO_MODEL is True:\n",
        "                file_without_extension = os.path.splitext(os.path.basename(path))[0]\n",
        "                frame_file_name = \"{0}{1}-{2}\".format(YOLO_FRAMES, file_without_extension, progress)\n",
        "\n",
        "                if os.path.isfile(frame_file_name + \".jpg\") is False:\n",
        "                    cv2.imwrite(frame_file_name + \".jpg\", clean_frame)\n",
        "\n",
        "                # Coordinates data\n",
        "                if os.path.isfile(frame_file_name + \".txt\"):\n",
        "                    coord_file = open(frame_file_name + \".txt\", \"a\")\n",
        "                    coord_file.write(\"0 {0} {1} {2} {3}\\n\".format((x + 10) / IMG_SIZE, (y + 10) / IMG_SIZE,\n",
        "                                                                  w / IMG_SIZE, h / IMG_SIZE))\n",
        "                    coord_file.close()\n",
        "                else:\n",
        "                    coord_file = open(frame_file_name + \".txt\", \"w\")\n",
        "                    coord_file.write(\"0 {0} {1} {2} {3}\\n\".format((x + 10) / IMG_SIZE, (y + 10) / IMG_SIZE,\n",
        "                                                                  w / IMG_SIZE, h / IMG_SIZE))\n",
        "                    coord_file.close()\n",
        "\n",
        "            cv2.rectangle(current_frame, (x, y), (x + w, y + h), (0, 0, 255), 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFqS50t_lMtW"
      },
      "source": [
        "Helpers for data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI_Cy5z6lRY9"
      },
      "outputs": [],
      "source": [
        "def cleanup_frames_dir():\n",
        "    \"\"\"\n",
        "    Deletes all files and directories inside YOLO_FRAMES directory\n",
        "    \"\"\"\n",
        "    # Deletes files\n",
        "    for frame in os.listdir(YOLO_FRAMES):\n",
        "        file_path = os.path.join(YOLO_FRAMES, frame)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "                os.unlink(file_path)\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "        except Exception as e:\n",
        "            print(\"Deletion failed {0} due to:\\n {1}\".format(file_path, e))\n",
        "\n",
        "\n",
        "def cleanup_data_path_txt():\n",
        "    \"\"\"\n",
        "    Removes all path string from the YOLO_MAIN_DATA YOLO_VALID_DATA YOLO_TEST_DATA\n",
        "    \"\"\"\n",
        "    # Remove the path strings from the main, valid and test files\n",
        "    open(YOLO_MAIN_DATA, \"w\").close()\n",
        "    open(YOLO_VALID_DATA, \"w\").close()\n",
        "    open(YOLO_TEST_DATA, \"w\").close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGDlNmPtwrnB"
      },
      "outputs": [],
      "source": [
        "print(os.listdir(YOLO_FRAMES))\n",
        "print(os.listdir(NUMPY_SAVE))\n",
        "# When processed data are present, skip\n",
        "# data processing to prevent unnecessary\n",
        "# computational time\n",
        "if len(os.listdir(YOLO_FRAMES)) != 0 and len(os.listdir(NUMPY_SAVE)) != 0:\n",
        "  SKIP_DATA_PROCESSING = True\n",
        "\n",
        "if SKIP_DATA_PROCESSING is False:\n",
        "  cleanup_frames_dir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI5YV2HmrHLD"
      },
      "source": [
        "## **Allocation of Data into Training and Valid sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wLt_NoprJlq"
      },
      "outputs": [],
      "source": [
        "# When processed data are present, skip\n",
        "# data processing to prevent unnecessary\n",
        "# computational time\n",
        "if len(os.listdir(YOLO_FRAMES)) != 0 and len(os.listdir(NUMPY_SAVE)) != 0:\n",
        "  SKIP_DATA_PROCESSING = True\n",
        "\n",
        "if SKIP_DATA_PROCESSING is False:  \n",
        "  # Process all the Data files from PATH_DATA\n",
        "  data_filenames = next(os.walk(os.path.join(PATH_DATA)), (None, None, []))[2]\n",
        "  data = ()\n",
        "  list_data = []\n",
        "  for file in data_filenames:\n",
        "      print(file)\n",
        "      data = load_video(PATH_DATA + file)\n",
        "      list_data.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8YQf8_SmbJm"
      },
      "outputs": [],
      "source": [
        "# When processed data are present, skip\n",
        "# data processing to prevent unnecessary\n",
        "# computational time\n",
        "if len(os.listdir(YOLO_FRAMES)) != 0 and len(os.listdir(NUMPY_SAVE)) != 0:\n",
        "  SKIP_DATA_PROCESSING = True\n",
        "\n",
        "if SKIP_DATA_PROCESSING is False:\n",
        "  # Create a numpy array from the processed data\n",
        "  imaging_data = []\n",
        "  labels = []\n",
        "  for data in list_data:\n",
        "      (x_train, y_train) = data\n",
        "      imaging_data = imaging_data + x_train\n",
        "      labels = labels + y_train\n",
        "  imaging_data = np.array(imaging_data)\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  dataset_size = len(labels)\n",
        "\n",
        "  print(\"Data Set Size:\", dataset_size)\n",
        "  print(\"Data: {0},  Labels: {1}\".format(imaging_data.shape, labels.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95TDFySEZaHR"
      },
      "outputs": [],
      "source": [
        "# When processed data are present, skip\n",
        "# data processing to prevent unnecessary\n",
        "# computational time\n",
        "if len(os.listdir(YOLO_FRAMES)) != 0 and len(os.listdir(NUMPY_SAVE)) != 0:\n",
        "  SKIP_DATA_PROCESSING = True\n",
        "\n",
        "if SKIP_DATA_PROCESSING is False:  \n",
        "  # Randomly allocate data for RES_NET_MODEL and MOBILENET_MODEL\n",
        "  train_imaging_data, valid_imaging_data, train_labels, valid_labels = train_test_split(imaging_data, labels, test_size=0.2)\n",
        "  print(\"train_imaging_data:\", train_imaging_data.shape)\n",
        "  print(\"train_labels:\", train_labels.shape)\n",
        "  print(\"valid_imaging_data:\", valid_imaging_data.shape)\n",
        "  print(\"valid_labels:\", valid_labels.shape)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  train_imaging_data, test_imaging_data, train_labels, test_labels = train_test_split(train_imaging_data, train_labels, test_size=0.2)\n",
        "  print(\"train_imaging_data:\", train_imaging_data.shape)\n",
        "  print(\"train_labels:\", train_labels.shape)\n",
        "  print(\"valid_imaging_data:\", valid_imaging_data.shape)\n",
        "  print(\"valid_labels:\", valid_labels.shape)\n",
        "  print(\"test_imaging_data:\", test_imaging_data.shape)\n",
        "  print(\"test_labels:\", test_labels.shape)\n",
        "\n",
        "  np.save(NUMPY_SAVE + \"train_imaging_data\", train_imaging_data)\n",
        "  np.save(NUMPY_SAVE + \"train_labels\", train_labels)\n",
        "  np.save(NUMPY_SAVE + \"valid_imaging_data\", valid_imaging_data)\n",
        "  np.save(NUMPY_SAVE + \"valid_labels\", valid_labels)\n",
        "  np.save(NUMPY_SAVE + \"test_imaging_data\", test_imaging_data)\n",
        "  np.save(NUMPY_SAVE + \"test_labels\", test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIHC0NHzyiIv"
      },
      "outputs": [],
      "source": [
        "if SKIP_DATA_PROCESSING is False:\n",
        "  # Clear unnecessary memory\n",
        "  del train_imaging_data\n",
        "  del train_labels\n",
        "  del valid_imaging_data\n",
        "  del valid_labels\n",
        "  del test_imaging_data\n",
        "  del test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiBd36Q7e1qU"
      },
      "outputs": [],
      "source": [
        "numpy_yolo_frames = np.asarray(os.listdir(YOLO_FRAMES))\n",
        "print(numpy_yolo_frames.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzcgjNBReas8"
      },
      "outputs": [],
      "source": [
        "yolo_main_set, yolo_validate_set, _, _ = train_test_split(numpy_yolo_frames, numpy_yolo_frames, test_size =0.2)\n",
        "print(\"yolo_main_set\", yolo_main_set.shape)\n",
        "print(\"yolo_validate_set\", yolo_validate_set.shape)\n",
        "print(\"-----------------------------------------------\")\n",
        "yolo_main_set, yolo_test_set, _, _ = train_test_split(yolo_main_set, yolo_main_set, test_size =0.2)\n",
        "print(\"yolo_main_set\", yolo_main_set.shape)\n",
        "print(\"yolo_validate_set\", yolo_validate_set.shape)\n",
        "print(\"yolo_test_set\", yolo_test_set.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_YAtFKXZoqh"
      },
      "outputs": [],
      "source": [
        "# Randomly allocate data for validation for YOLO_MODEL\n",
        "if YOLO_MODEL is True:\n",
        "    # Updates the data paths for each set of text files\n",
        "    cleanup_data_path_txt()\n",
        "\n",
        "    # YOLO_MAIN_DATA\n",
        "    for main_data in yolo_main_set:\n",
        "        if main_data.endswith(\".jpg\"):\n",
        "            if os.path.isfile(YOLO_MAIN_DATA):\n",
        "                open(YOLO_MAIN_DATA, \"a\").write(\"{0}{1}\\n\".format(YOLO_FRAMES, main_data))\n",
        "            else:\n",
        "                open(YOLO_MAIN_DATA, \"w\").write(\"{0}{1}\\n\".format(YOLO_FRAMES, main_data))\n",
        "\n",
        "    # YOLO_VALID_DATA\n",
        "    for valid_data in yolo_validate_set:\n",
        "        if valid_data.endswith(\".jpg\"):\n",
        "            if os.path.isfile(YOLO_VALID_DATA):\n",
        "                open(YOLO_VALID_DATA, \"a\").write(\"{0}{1}\\n\".format(YOLO_FRAMES, valid_data))\n",
        "            else:\n",
        "                open(YOLO_VALID_DATA, \"w\").write(\"{0}{1}\\n\".format(YOLO_FRAMES, valid_data))\n",
        "\n",
        "    # YOLO_TEST_DATA\n",
        "    for test_data in yolo_test_set:\n",
        "        if test_data.endswith(\".jpg\"):\n",
        "            if os.path.isfile(YOLO_TEST_DATA):\n",
        "                open(YOLO_TEST_DATA, \"a\").write(\"{0}{1}\\n\".format(YOLO_FRAMES, test_data))\n",
        "            else:\n",
        "                open(YOLO_TEST_DATA, \"w\").write(\"{0}{1}\\n\".format(YOLO_FRAMES, test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxQssLbmaImy"
      },
      "source": [
        "# DATA MANIPULATION AND TRAINING FOR ML MODELS\n",
        "# &\n",
        "# ANALYTICS FOR ML MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgcPNFt3oxtN"
      },
      "source": [
        "### **YOLO Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suTI93Sbm3nd"
      },
      "source": [
        "Setup Darknet framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W27WS23hGUUy"
      },
      "outputs": [],
      "source": [
        "!rm -rf darknet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtw7rZrbD6jX"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AlexeyAB/darknet.git\n",
        "\n",
        "%cd darknet\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
        "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile\n",
        "%cd ..\n",
        "\n",
        "!cd darknet/ && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34tfzcdrFcVO"
      },
      "outputs": [],
      "source": [
        "!cd darknet && wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov3.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVtq9tLMG3RJ"
      },
      "outputs": [],
      "source": [
        "# verify CUDA\n",
        "!/usr/local/cuda/bin/nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZgj8sxuIJwr"
      },
      "outputs": [],
      "source": [
        "!rm darknet/cfg/custom-yolov3-tiny.cfg\n",
        "!cd darknet && cp cfg/yolov3-tiny.cfg cfg/custom-yolov3-tiny.cfg\n",
        "\n",
        "%cd darknet/cfg\n",
        "!sed -i 's/batch=1/batch='{YOLO_BATCH}'/' custom-yolov3-tiny.cfg\n",
        "!sed -i 's/subdivisions=1/subdivisions='{YOLO_SUBDIVISION}'/' custom-yolov3-tiny.cfg\n",
        "!sed -i 's/max_batches = 500200/max_batches = '{YOLO_MAX_BATCHES}'/' custom-yolov3-tiny.cfg\n",
        "!sed -i 's/steps=400000,450000/steps='{YOLO_LOWER_STEPS}','{YOLO_UPPER_STEPS}'/' custom-yolov3-tiny.cfg\n",
        "!sed -i 's/classes=80/classes='{len(CLASSES)}'/' custom-yolov3-tiny.cfg\n",
        "!sed -i 's/filters=255/filters='{(len(CLASSES)+5)*3}'/' custom-yolov3-tiny.cfg\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxE6zW_EiydC"
      },
      "outputs": [],
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3-tiny.weights -P darknet/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqHFMkrCOT0r"
      },
      "outputs": [],
      "source": [
        "!echo {YOLO_DATA_FILE}\n",
        "!ls -al darknet/cfg/ | grep -i custom-yolov3-tiny.cfg\n",
        "!ls -al darknet/ | grep -i yolov3-tiny.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n7lUy8Em9Cg"
      },
      "source": [
        "Run YOLO model in Darknet framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J-hMLCni50J"
      },
      "outputs": [],
      "source": [
        "if YOLO_MODEL:\n",
        "  # Start from fresh\n",
        "  # !cd darknet && ./darknet detector train {YOLO_DATA_FILE} cfg/custom-yolov3-tiny.cfg yolov3-tiny.weights -dont_show -map\n",
        "\n",
        "  # Continue from the lastest weights\n",
        "  !cd darknet && ./darknet detector train {YOLO_DATA_FILE} cfg/custom-yolov3-tiny.cfg {YOLO_BACKUP}custom-yolov3-tiny_last.weights -dont_show -map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvFGeF4Oo9El"
      },
      "source": [
        "### **YOLO Model Loss and mAP Graph**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTKt4eMznFtM"
      },
      "source": [
        "YOLO model analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97rnhiZYlsOY"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  if YOLO_MODEL:\n",
        "    # img = cv2.imread(\"darknet/chart.png\")\n",
        "    # cv2_imshow(img)\n",
        "\n",
        "    image = cv2.imread(\"darknet/chart.png\")\n",
        "    print(image.shape)\n",
        "    height, width = image.shape[:2]\n",
        "    resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "    fig = plt.gcf()\n",
        "    fig.set_size_inches(18, 10)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "except AttributeError:\n",
        "  raise AttributeError(\"chart.png Has Not Been Detected in darknet/ Directory, Maybe Let the YOLO Model Run More Longer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hks01CEsflL"
      },
      "outputs": [],
      "source": [
        "if YOLO_MODEL:\n",
        "  for yolo_test in yolo_test_set:\n",
        "    if yolo_test.endswith(\".jpg\"):\n",
        "      !cd darknet && ./darknet detector test {YOLO_DATA_FILE} cfg/custom-yolov3-tiny.cfg {YOLO_BACKUP}custom-yolov3-tiny_last.weights {YOLO_FRAMES}{yolo_test}\n",
        "      \n",
        "      # Demonstration Purposes Only\n",
        "      # img = cv2.imread(\"darknet/predictions.jpg\")\n",
        "      # cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXzeMML6zTHt"
      },
      "source": [
        "Copy the final model weights for YOLO Model to models/ directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxHdEpfjw2ub"
      },
      "outputs": [],
      "source": [
        "!mkdir {FINAL_MODELS_PATH}yolo/\n",
        "!cp {YOLO_BACKUP}custom-yolov3-tiny_last.weights {FINAL_MODELS_PATH}yolo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agbFmvCyy2hl"
      },
      "outputs": [],
      "source": [
        "train_imaging_data = np.load(NUMPY_SAVE + \"train_imaging_data.npy\")\n",
        "train_labels = np.load(NUMPY_SAVE + \"train_labels.npy\")\n",
        "valid_imaging_data = np.load(NUMPY_SAVE + \"valid_imaging_data.npy\")\n",
        "valid_labels = np.load(NUMPY_SAVE + \"valid_labels.npy\")\n",
        "test_imaging_data = np.load(NUMPY_SAVE + \"test_imaging_data.npy\")\n",
        "test_labels = np.load(NUMPY_SAVE + \"test_labels.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH0UjIZN2WNZ"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=len(CLASSES))\n",
        "valid_labels = tf.keras.utils.to_categorical(valid_labels, num_classes=len(CLASSES))\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=len(CLASSES))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cfauiDfN60T"
      },
      "source": [
        "### **ResNet Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLnAfrcbygXd"
      },
      "outputs": [],
      "source": [
        "log_dir = \"logs/fit/resnet_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(BACKUP_TENSOR + \"resnet_model\",\n",
        "                                                  save_weights_only=True,\n",
        "                                                  verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5EFc9cpaZdX"
      },
      "outputs": [],
      "source": [
        "if RES_NET_MODEL is True:\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.applications.ResNet50V2(weights=\"imagenet\",\n",
        "                                          include_top=False,\n",
        "                                          input_shape=INPUT_SHAPE,\n",
        "                                          classes=len(CLASSES)),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(len(CLASSES), activation=\"softmax\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    \n",
        "    resnet_train_imaging_data = train_imaging_data\n",
        "    resnet_train_labels = train_labels\n",
        "\n",
        "    resnet_valid_imaging_data = valid_imaging_data\n",
        "    resnet_valid_labels = valid_labels\n",
        "\n",
        "    # Demonstration Purposes Only\n",
        "    # cv2_imshow(resnet_train_imaging_data[0])\n",
        "    # print(\"Before Shape\", resnet_train_imaging_data[0].shape)\n",
        "\n",
        "    resnet_train_imaging_data = tf.keras.applications.resnet.preprocess_input(resnet_train_imaging_data)\n",
        "    resnet_valid_imaging_data = tf.keras.applications.resnet.preprocess_input(resnet_valid_imaging_data)\n",
        "\n",
        "    # Demonstration Purposes Only\n",
        "    # cv2_imshow(resnet_train_imaging_data[0])\n",
        "    # print(\"After Shape\", resnet_train_imaging_data[0].shape)\n",
        "\n",
        "    history_res_net_model = model.fit(resnet_train_imaging_data, resnet_train_labels, \n",
        "                                      batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
        "                                      validation_data=(resnet_valid_imaging_data, resnet_valid_labels),\n",
        "                                      callbacks=[tensorboard_callback, cp_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vn1wE-Czk2j"
      },
      "source": [
        "Copy the final model weights for ResNet Model to models/ directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzEvVmWo0duB"
      },
      "outputs": [],
      "source": [
        "if RES_NET_MODEL:\n",
        "  model.save(FINAL_MODELS_PATH + \"resnet_model/resnet_model.h5\")\n",
        "  model = tf.keras.models.load_model(FINAL_MODELS_PATH + \"resnet_model/resnet_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgtXur36103V"
      },
      "outputs": [],
      "source": [
        "if RES_NET_MODEL:\n",
        "  print(history_res_net_model.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FIXndSIG7M6"
      },
      "source": [
        " ### ResNet Model Loss Over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2xQYTNtG394"
      },
      "outputs": [],
      "source": [
        "if RES_NET_MODEL:\n",
        "  history_res_net_model_dict = history_res_net_model.history\n",
        "  loss_values = history_res_net_model_dict['loss']\n",
        "  val_loss_values = history_res_net_model_dict['val_loss']\n",
        "\n",
        "  epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "  plt.plot(epochs, loss_values, 'g', label='Training loss') # blue dots\n",
        "  plt.plot(epochs, val_loss_values, 'b', label='Validation loss') #Â blue line\n",
        "  plt.title(\"Training and Validation Loss Over Epochs\")\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nSGxCIvJVP8"
      },
      "source": [
        "### ResNet Model Accuracy Over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqI_J5gDJt3a"
      },
      "outputs": [],
      "source": [
        "if RES_NET_MODEL:\n",
        "  acc = history_res_net_model_dict['accuracy']\n",
        "  val_acc = history_res_net_model_dict['val_accuracy']\n",
        "\n",
        "  plt.plot(epochs, acc, 'g', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and Validation Accuracy Over Epochs')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkqWgjdrLHgM"
      },
      "source": [
        "### ResNet Model Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvViezWVLFRS"
      },
      "outputs": [],
      "source": [
        "if RES_NET_MODEL:\n",
        "  pred_labels = model.predict(test_imaging_data)\n",
        "  matrix = metrics.confusion_matrix(np.argmax(test_labels, axis=1), np.argmax(pred_labels, axis=1))\n",
        "  print(matrix)\n",
        "\n",
        "  plot_confusion_matrix(conf_mat=matrix, figsize=(8, 8), \n",
        "                        colorbar=True, show_absolute=False, \n",
        "                        show_normed=True,\n",
        "                        class_names=CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUrHAFU9qGGr"
      },
      "outputs": [],
      "source": [
        "if RES_NET_MODEL:\n",
        "  # Clear unnecessary memory\n",
        "  del resnet_train_imaging_data\n",
        "  del resnet_train_labels\n",
        "  del resnet_valid_imaging_data\n",
        "  del resnet_valid_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gznGvhhWODVG"
      },
      "source": [
        "### **MobileNetV2 Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LONckyya-N0Y"
      },
      "outputs": [],
      "source": [
        "log_dir = \"logs/fit/mobilenet_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(BACKUP_TENSOR + \"mobilenet_model\",\n",
        "                                                  save_weights_only=True,\n",
        "                                                  verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K-anpM9OG68"
      },
      "outputs": [],
      "source": [
        "if MOBILENET_MODEL is True:\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.applications.MobileNetV2(weights=\"imagenet\",\n",
        "                                          include_top=False,\n",
        "                                          input_shape=INPUT_SHAPE,\n",
        "                                          classes=len(CLASSES)),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(len(CLASSES), activation=\"softmax\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    mobilenet_train_imaging_data = train_imaging_data\n",
        "    mobilenet_train_labels = train_labels\n",
        "\n",
        "    mobilenet_valid_imaging_data = valid_imaging_data\n",
        "    mobilenet_valid_labels = valid_labels\n",
        "\n",
        "    # Demonstration Purposes Only\n",
        "    # cv2_imshow(mobilenet_train_imaging_data[0])\n",
        "    # print(\"Before Shape: \", mobilenet_train_imaging_data[0].shape)\n",
        "\n",
        "    mobilenet_train_imaging_data = tf.keras.applications.mobilenet.preprocess_input(mobilenet_train_imaging_data)\n",
        "    mobilenet_valid_imaging_data = tf.keras.applications.mobilenet.preprocess_input(mobilenet_valid_imaging_data)\n",
        "\n",
        "    # Demonstration Purposes Only\n",
        "    # cv2_imshow(mobilenet_train_imaging_data[0])\n",
        "    # print(\"After Shape\", mobilenet_train_imaging_data[0].shape)\n",
        "\n",
        "    history_mobilenetv2_model = model.fit(mobilenet_train_imaging_data, mobilenet_train_labels,\n",
        "                        batches=BATCH_SIZE, epochs=EPOCHS, \n",
        "                        validation_data=(mobilenet_valid_imaging_data, mobilenet_valid_labels),\n",
        "                        callbacks=[tensorboard_callback, cp_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKHCZwmjzgEW"
      },
      "source": [
        "Copy the final model weights for MobileNet Model to models/ directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF6CAQoN-vAt"
      },
      "outputs": [],
      "source": [
        "if MOBILENET_MODEL:\n",
        "  model.save(FINAL_MODELS_PATH + \"mobilenet_model/mobilenet_model.h5\")\n",
        "  model = tf.keras.models.load_model(FINAL_MODELS_PATH + \"mobilenet_model/mobilenet_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVsB7-ojbNo3"
      },
      "outputs": [],
      "source": [
        "if MOBILENET_MODEL:\n",
        "  print(history_mobilenetv2_model.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx5oAXa5Og8w"
      },
      "source": [
        " ### MobileNetV2 Model Loss Over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUxFAoj5Oj-X"
      },
      "outputs": [],
      "source": [
        "if MOBILENET_MODEL:\n",
        "  history_mobilenetv2_model_dict = history_mobilenetv2_model.history\n",
        "  loss_values = history_mobilenetv2_model_dict['loss']\n",
        "  val_loss_values = history_mobilenetv2_model_dict['val_loss']\n",
        "\n",
        "  epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "  plt.plot(epochs, loss_values, 'g', label='Training loss') # blue dots\n",
        "  plt.plot(epochs, val_loss_values, 'b', label='Validation loss') #Â blue line\n",
        "  plt.title(\"Training and Validation Loss Over Epochs\")\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBfWx3yIQgZh"
      },
      "source": [
        "### MobileNetV2 Model Accuracy Over Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQuCddWtQrBt"
      },
      "outputs": [],
      "source": [
        "if MOBILENET_MODEL:\n",
        "  acc = history_mobilenetv2_model_dict['accuracy']\n",
        "  val_acc = history_mobilenetv2_model_dict['val_accuracy']\n",
        "\n",
        "  plt.plot(epochs, acc, 'g', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and Validation Accuracy Over Epochs')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSCWCCbT0XEV"
      },
      "source": [
        "### MobileNetV2 Model Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1sbKlQ20QaG"
      },
      "outputs": [],
      "source": [
        "if MOBILENET_MODEL:\n",
        "  pred_labels = model.predict(test_imaging_data)\n",
        "  matrix = metrics.confusion_matrix(np.argmax(test_labels, axis=1), np.argmax(pred_labels, axis=1))\n",
        "\n",
        "  plot_confusion_matrix(conf_mat=matrix, figsize=(8, 8), \n",
        "                        colorbar=True, show_absolute=False, \n",
        "                        show_normed=True,\n",
        "                        class_names=CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTOs9uWIqdox"
      },
      "outputs": [],
      "source": [
        "if MOBILENET_MODEL:\n",
        "  # Clear unnecessary memory\n",
        "  del mobilenet_train_imaging_data\n",
        "  del mobilenet_train_labels\n",
        "  del mobilenet_valid_imaging_data\n",
        "  del mobilenet_valid_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOvq_lwDb3ST"
      },
      "outputs": [],
      "source": [
        "# Clear unnecessary memory\n",
        "del train_imaging_data\n",
        "del train_labels\n",
        "del valid_imaging_data\n",
        "del valid_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB0jtZcDm7gU"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkqIXM3IMEmn"
      },
      "outputs": [],
      "source": [
        "# Clean up Logs\n",
        "# !rm -rf logs/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Software_Stack_Program-Abdullah_Alshadadi.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
